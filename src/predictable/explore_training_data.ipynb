{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Training Data\n",
    "\n",
    "This notebook visualizes samples from the training data to understand the input-output relationship."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:39:46.019858Z",
     "start_time": "2025-10-27T19:39:46.013409Z"
    }
   },
   "source": "import numpy as np\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n# Load training data\ndata_dir = Path(\"data\")\ntrain_hidden_states = np.load(data_dir / \"train_hidden_states.npy\")\ntrain_remaining_tokens = np.load(data_dir / \"train_remaining_tokens.npy\")\ntrain_token_metadata = np.load(data_dir / \"train_token_metadata.npy\")\n\nprint(f\"Training data shape:\")\nprint(f\"Hidden states: {train_hidden_states.shape}\")\nprint(f\"Remaining tokens: {train_remaining_tokens.shape}\")\nprint(f\"Token metadata: {train_token_metadata.shape}\")\nprint(f\"\\nTotal samples: {len(train_hidden_states):,}\")\nprint(f\"\\nMetadata fields: {train_token_metadata.dtype.names}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:\n",
      "Hidden states: (99, 3072)\n",
      "Remaining tokens: (99,)\n",
      "Token metadata: (99,)\n",
      "\n",
      "Total samples: 99\n",
      "\n",
      "Metadata fields: ('token_id', 'token_text')\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data Points\n",
    "\n",
    "Let's look at a few random samples to see the input-output pairs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:39:49.649590Z",
     "start_time": "2025-10-27T19:39:49.642666Z"
    }
   },
   "source": "# Show 10 random samples\nnp.random.seed(42)\nsample_indices = range(10)\n\nprint(\"Random samples from training data:\")\nprint(\"=\"*60)\nfor i, idx in enumerate(sample_indices):\n    hidden_state = train_hidden_states[idx]\n    remaining = train_remaining_tokens[idx]\n    token_id = train_token_metadata[idx]['token_id']\n    token_text = train_token_metadata[idx]['token_text']\n    \n    print(f\"\\nSample {i+1} (index {idx}):\")\n    print(f\"  Token: '{token_text}' (id={token_id})\")\n    print(f\"  Input (hidden state): shape={hidden_state.shape}, norm={np.linalg.norm(hidden_state):.4f}\")\n    print(f\"  First 10 values: {hidden_state[:10]}\")\n    print(f\"  Output (remaining tokens): {remaining}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random samples from training data:\n",
      "============================================================\n",
      "\n",
      "Sample 1 (index 0):\n",
      "  Token: '\n",
      "\n",
      "' (id=271)\n",
      "  Input (hidden state): shape=(3072,), norm=89.6875\n",
      "  First 10 values: [-1.068   1.066   1.274   0.2205  1.554  -0.8564 -1.797   1.861  -0.986\n",
      " -0.9805]\n",
      "  Output (remaining tokens): 10\n",
      "\n",
      "Sample 2 (index 1):\n",
      "  Token: 'hello' (id=15339)\n",
      "  Input (hidden state): shape=(3072,), norm=89.9375\n",
      "  First 10 values: [ 2.729    2.598    0.02315  1.398   -0.629   -0.196   -1.846   -0.7637\n",
      "  0.4102  -1.384  ]\n",
      "  Output (remaining tokens): 9\n",
      "\n",
      "Sample 3 (index 2):\n",
      "  Token: '\n",
      "' (id=198)\n",
      "  Input (hidden state): shape=(3072,), norm=86.6875\n",
      "  First 10 values: [-0.604  1.889  1.465  1.266  1.081 -1.03  -3.016 -0.745  1.162 -1.031]\n",
      "  Output (remaining tokens): 8\n",
      "\n",
      "Sample 4 (index 3):\n",
      "  Token: 'hello' (id=15339)\n",
      "  Input (hidden state): shape=(3072,), norm=89.3750\n",
      "  First 10 values: [-1.184   0.9717 -0.1624  0.647   0.2041  0.4475 -1.611   0.6177 -0.157\n",
      " -1.122 ]\n",
      "  Output (remaining tokens): 7\n",
      "\n",
      "Sample 5 (index 4):\n",
      "  Token: '\n",
      "' (id=198)\n",
      "  Input (hidden state): shape=(3072,), norm=84.1250\n",
      "  First 10 values: [ 0.2917  1.675   1.828   1.454   0.9253 -1.476  -2.217   1.079   1.467\n",
      " -1.352 ]\n",
      "  Output (remaining tokens): 6\n",
      "\n",
      "Sample 6 (index 5):\n",
      "  Token: 'hello' (id=15339)\n",
      "  Input (hidden state): shape=(3072,), norm=89.9375\n",
      "  First 10 values: [-2.873    0.03244 -1.574    0.4062  -0.7817   0.9126  -1.8545   1.139\n",
      " -0.9443  -0.983  ]\n",
      "  Output (remaining tokens): 5\n",
      "\n",
      "Sample 7 (index 6):\n",
      "  Token: '\n",
      "' (id=198)\n",
      "  Input (hidden state): shape=(3072,), norm=85.1875\n",
      "  First 10 values: [ 0.7017  2.61    1.978   2.027   0.8193 -0.3608 -2.219   1.027   1.522\n",
      " -1.768 ]\n",
      "  Output (remaining tokens): 4\n",
      "\n",
      "Sample 8 (index 7):\n",
      "  Token: 'hello' (id=15339)\n",
      "  Input (hidden state): shape=(3072,), norm=89.7500\n",
      "  First 10 values: [-2.51     0.0865  -1.316    0.12024 -0.1895   1.22    -1.674    1.302\n",
      " -0.9155  -0.968  ]\n",
      "  Output (remaining tokens): 3\n",
      "\n",
      "Sample 9 (index 8):\n",
      "  Token: '\n",
      "' (id=198)\n",
      "  Input (hidden state): shape=(3072,), norm=83.4375\n",
      "  First 10 values: [ 0.301   2.805   1.698   1.688   0.738  -0.3677 -2.227   1.733   1.117\n",
      " -1.389 ]\n",
      "  Output (remaining tokens): 2\n",
      "\n",
      "Sample 10 (index 9):\n",
      "  Token: 'hello' (id=15339)\n",
      "  Input (hidden state): shape=(3072,), norm=89.7500\n",
      "  First 10 values: [-2.402    0.6855  -1.258   -0.06067 -0.00872  1.559   -1.672    2.72\n",
      " -0.0895  -1.14   ]\n",
      "  Output (remaining tokens): 1\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Remaining Tokens\n",
    "\n",
    "Let's visualize the distribution of the target variable (remaining tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train_remaining_tokens, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Remaining Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Remaining Tokens')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Statistics\n",
    "plt.subplot(1, 2, 2)\n",
    "stats_text = f\"\"\"Statistics:\n",
    "Min: {train_remaining_tokens.min()}\n",
    "Max: {train_remaining_tokens.max()}\n",
    "Mean: {train_remaining_tokens.mean():.2f}\n",
    "Median: {np.median(train_remaining_tokens):.2f}\n",
    "Std: {train_remaining_tokens.std():.2f}\n",
    "\n",
    "Unique values: {len(np.unique(train_remaining_tokens))}\n",
    "\"\"\"\n",
    "plt.text(0.1, 0.5, stats_text, fontsize=12, family='monospace',\n",
    "         verticalalignment='center')\n",
    "plt.axis('off')\n",
    "plt.title('Target Variable Statistics')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden State Analysis\n",
    "\n",
    "Let's examine the hidden state vectors to understand their properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics across all hidden states\n",
    "hidden_norms = np.linalg.norm(train_hidden_states, axis=1)\n",
    "hidden_means = train_hidden_states.mean(axis=1)\n",
    "hidden_stds = train_hidden_states.std(axis=1)\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "# Norms\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(hidden_norms, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('L2 Norm')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Hidden State Norms')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Means\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(hidden_means, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Mean Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Hidden State Means')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Stds\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(hidden_stds, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Standard Deviation')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Hidden State Std Devs')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship: Hidden States vs Remaining Tokens\n",
    "\n",
    "Let's explore if there's any obvious relationship between hidden state properties and remaining tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "# Norm vs Remaining Tokens\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(train_remaining_tokens, hidden_norms, alpha=0.1, s=1)\n",
    "plt.xlabel('Remaining Tokens')\n",
    "plt.ylabel('Hidden State Norm')\n",
    "plt.title('Norm vs Remaining Tokens')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Mean vs Remaining Tokens\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(train_remaining_tokens, hidden_means, alpha=0.1, s=1)\n",
    "plt.xlabel('Remaining Tokens')\n",
    "plt.ylabel('Hidden State Mean')\n",
    "plt.title('Mean vs Remaining Tokens')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Std vs Remaining Tokens\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(train_remaining_tokens, hidden_stds, alpha=0.1, s=1)\n",
    "plt.xlabel('Remaining Tokens')\n",
    "plt.ylabel('Hidden State Std Dev')\n",
    "plt.title('Std Dev vs Remaining Tokens')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Sequence Analysis\n",
    "\n",
    "Let's look at how remaining tokens decrease during generation for specific examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Find samples that form sequences (consecutive remaining token counts)\n# This assumes the data was generated sequentially before shuffling\n\nprint(\"Examples of how remaining tokens change during generation:\")\nprint(\"=\"*60)\n\n# Show first 20 samples with their tokens\nprint(\"\\nFirst 20 samples (may show part of a generation sequence):\")\nfor i in range(min(20, len(train_remaining_tokens))):\n    token_text = train_token_metadata[i]['token_text']\n    token_id = train_token_metadata[i]['token_id']\n    print(f\"Sample {i:3d}: token='{token_text:20s}' (id={token_id:6d}), remaining={train_remaining_tokens[i]:3d}\")\n\n# Find sequences where remaining tokens decrease by 1\nprint(\"\\n\\nLooking for consecutive decreasing sequences...\")\nsequence_starts = []\nfor i in range(len(train_remaining_tokens) - 5):\n    # Check if we have a decreasing sequence\n    is_sequence = all(\n        train_remaining_tokens[i+j] - train_remaining_tokens[i+j+1] == 1\n        for j in range(4)\n    )\n    if is_sequence:\n        sequence_starts.append(i)\n        if len(sequence_starts) >= 3:  # Show first 3 found\n            break\n\nif sequence_starts:\n    for seq_start in sequence_starts:\n        print(f\"\\nSequence starting at index {seq_start}:\")\n        for j in range(10):\n            if seq_start + j < len(train_remaining_tokens):\n                token_text = train_token_metadata[seq_start + j]['token_text']\n                print(f\"  Step {j}: token='{token_text:20s}' remaining={train_remaining_tokens[seq_start + j]}\")\nelse:\n    print(\"No obvious sequences found (data is shuffled)\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

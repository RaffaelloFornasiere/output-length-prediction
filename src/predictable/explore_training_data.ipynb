{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Training Data\n",
    "\n",
    "This notebook visualizes samples from the training data to understand the input-output relationship."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:27:16.872212Z",
     "start_time": "2025-10-27T18:27:16.788272Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load training data\n",
    "data_dir = Path(\"data\")\n",
    "train_hidden_states = np.load(data_dir / \"hidden_states.npy\")\n",
    "train_remaining_tokens = np.load(data_dir / \"remaining_tokens.npy\")\n",
    "\n",
    "print(f\"Training data shape:\")\n",
    "print(f\"Hidden states: {train_hidden_states.shape}\")\n",
    "print(f\"Remaining tokens: {train_remaining_tokens.shape}\")\n",
    "print(f\"\\nTotal samples: {len(train_hidden_states):,}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:\n",
      "Hidden states: (59723, 3072)\n",
      "Remaining tokens: (59723,)\n",
      "\n",
      "Total samples: 59,723\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data Points\n",
    "\n",
    "Let's look at a few random samples to see the input-output pairs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:27:29.662850Z",
     "start_time": "2025-10-27T18:27:29.657345Z"
    }
   },
   "source": [
    "# Show 10 random samples\n",
    "np.random.seed(42)\n",
    "sample_indices = range(10)\n",
    "\n",
    "print(\"Random samples from training data:\")\n",
    "print(\"=\"*60)\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    hidden_state = train_hidden_states[idx]\n",
    "    remaining = train_remaining_tokens[idx]\n",
    "    \n",
    "    print(f\"\\nSample {i+1} (index {idx}):\")\n",
    "    print(f\"  Input (hidden state): shape={hidden_state.shape}, norm={np.linalg.norm(hidden_state):.4f}\")\n",
    "    print(f\"  First 10 values: {hidden_state[:10]}\")\n",
    "    print(f\"  Output (remaining tokens): {remaining}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random samples from training data:\n",
      "============================================================\n",
      "\n",
      "Sample 1 (index 0):\n",
      "  Input (hidden state): shape=(3072,), norm=89.6875\n",
      "  First 10 values: [-0.3225 -1.882   5.734  -1.665  -0.849   2.258  -1.708   2.209  -1.11\n",
      " -0.557 ]\n",
      "  Output (remaining tokens): 199\n",
      "\n",
      "Sample 2 (index 1):\n",
      "  Input (hidden state): shape=(3072,), norm=86.5625\n",
      "  First 10 values: [-0.4038 -1.212   1.965  -0.355   0.1543  1.094  -0.616   1.7705  0.5596\n",
      " -0.8027]\n",
      "  Output (remaining tokens): 198\n",
      "\n",
      "Sample 3 (index 2):\n",
      "  Input (hidden state): shape=(3072,), norm=89.3750\n",
      "  First 10 values: [ 0.38     0.3936   2.469    0.3425  -0.2551   0.9487   0.09454  0.9873\n",
      "  1.181   -0.1696 ]\n",
      "  Output (remaining tokens): 197\n",
      "\n",
      "Sample 4 (index 3):\n",
      "  Input (hidden state): shape=(3072,), norm=86.5000\n",
      "  First 10 values: [ 0.3948 -1.099   0.6655  1.541  -0.4224  0.5884 -0.902   0.841   1.329\n",
      " -0.0683]\n",
      "  Output (remaining tokens): 196\n",
      "\n",
      "Sample 5 (index 4):\n",
      "  Input (hidden state): shape=(3072,), norm=88.5000\n",
      "  First 10 values: [ 1.46   -0.6304 -0.6675 -0.7837  1.074  -3.27    2.422  -0.1184  0.1895\n",
      "  0.7236]\n",
      "  Output (remaining tokens): 195\n",
      "\n",
      "Sample 6 (index 5):\n",
      "  Input (hidden state): shape=(3072,), norm=90.0000\n",
      "  First 10 values: [ 1.688  -0.5815  0.578   1.904  -0.6997 -2.918   2.81   -1.47   -1.134\n",
      " -0.3877]\n",
      "  Output (remaining tokens): 194\n",
      "\n",
      "Sample 7 (index 6):\n",
      "  Input (hidden state): shape=(3072,), norm=87.6875\n",
      "  First 10 values: [ 0.1371  1.662   0.3892 -2.137  -0.569  -2.594   1.301   1.394  -0.4238\n",
      "  0.8916]\n",
      "  Output (remaining tokens): 193\n",
      "\n",
      "Sample 8 (index 7):\n",
      "  Input (hidden state): shape=(3072,), norm=87.3125\n",
      "  First 10 values: [ 1.101    0.407    0.378   -1.737   -0.08594 -0.6465  -0.5137  -0.08685\n",
      "  1.132    0.575  ]\n",
      "  Output (remaining tokens): 192\n",
      "\n",
      "Sample 9 (index 8):\n",
      "  Input (hidden state): shape=(3072,), norm=89.9375\n",
      "  First 10 values: [ 1.181  -0.2666 -0.669   0.928   0.4954 -4.465   0.8945  1.761  -0.4666\n",
      " -0.2185]\n",
      "  Output (remaining tokens): 191\n",
      "\n",
      "Sample 10 (index 9):\n",
      "  Input (hidden state): shape=(3072,), norm=90.3125\n",
      "  First 10 values: [ 1.7295 -1.204   0.4631  0.9863 -1.373  -4.223   1.668  -0.4507 -1.247\n",
      "  0.789 ]\n",
      "  Output (remaining tokens): 190\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Remaining Tokens\n",
    "\n",
    "Let's visualize the distribution of the target variable (remaining tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train_remaining_tokens, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Remaining Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Remaining Tokens')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Statistics\n",
    "plt.subplot(1, 2, 2)\n",
    "stats_text = f\"\"\"Statistics:\n",
    "Min: {train_remaining_tokens.min()}\n",
    "Max: {train_remaining_tokens.max()}\n",
    "Mean: {train_remaining_tokens.mean():.2f}\n",
    "Median: {np.median(train_remaining_tokens):.2f}\n",
    "Std: {train_remaining_tokens.std():.2f}\n",
    "\n",
    "Unique values: {len(np.unique(train_remaining_tokens))}\n",
    "\"\"\"\n",
    "plt.text(0.1, 0.5, stats_text, fontsize=12, family='monospace',\n",
    "         verticalalignment='center')\n",
    "plt.axis('off')\n",
    "plt.title('Target Variable Statistics')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden State Analysis\n",
    "\n",
    "Let's examine the hidden state vectors to understand their properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics across all hidden states\n",
    "hidden_norms = np.linalg.norm(train_hidden_states, axis=1)\n",
    "hidden_means = train_hidden_states.mean(axis=1)\n",
    "hidden_stds = train_hidden_states.std(axis=1)\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "# Norms\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(hidden_norms, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('L2 Norm')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Hidden State Norms')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Means\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(hidden_means, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Mean Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Hidden State Means')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Stds\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(hidden_stds, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Standard Deviation')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Hidden State Std Devs')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship: Hidden States vs Remaining Tokens\n",
    "\n",
    "Let's explore if there's any obvious relationship between hidden state properties and remaining tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "# Norm vs Remaining Tokens\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(train_remaining_tokens, hidden_norms, alpha=0.1, s=1)\n",
    "plt.xlabel('Remaining Tokens')\n",
    "plt.ylabel('Hidden State Norm')\n",
    "plt.title('Norm vs Remaining Tokens')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Mean vs Remaining Tokens\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(train_remaining_tokens, hidden_means, alpha=0.1, s=1)\n",
    "plt.xlabel('Remaining Tokens')\n",
    "plt.ylabel('Hidden State Mean')\n",
    "plt.title('Mean vs Remaining Tokens')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Std vs Remaining Tokens\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(train_remaining_tokens, hidden_stds, alpha=0.1, s=1)\n",
    "plt.xlabel('Remaining Tokens')\n",
    "plt.ylabel('Hidden State Std Dev')\n",
    "plt.title('Std Dev vs Remaining Tokens')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Sequence Analysis\n",
    "\n",
    "Let's look at how remaining tokens decrease during generation for specific examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find samples that form sequences (consecutive remaining token counts)\n",
    "# This assumes the data was generated sequentially before shuffling\n",
    "\n",
    "print(\"Examples of how remaining tokens change during generation:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show first 20 samples\n",
    "print(\"\\nFirst 20 samples (may show part of a generation sequence):\")\n",
    "for i in range(20):\n",
    "    print(f\"Sample {i:3d}: remaining_tokens = {train_remaining_tokens[i]:3d}\")\n",
    "\n",
    "# Find sequences where remaining tokens decrease by 1\n",
    "print(\"\\n\\nLooking for consecutive decreasing sequences...\")\n",
    "sequence_starts = []\n",
    "for i in range(len(train_remaining_tokens) - 5):\n",
    "    # Check if we have a decreasing sequence\n",
    "    is_sequence = all(\n",
    "        train_remaining_tokens[i+j] - train_remaining_tokens[i+j+1] == 1\n",
    "        for j in range(4)\n",
    "    )\n",
    "    if is_sequence:\n",
    "        sequence_starts.append(i)\n",
    "        if len(sequence_starts) >= 3:  # Show first 3 found\n",
    "            break\n",
    "\n",
    "if sequence_starts:\n",
    "    for seq_start in sequence_starts:\n",
    "        print(f\"\\nSequence starting at index {seq_start}:\")\n",
    "        for j in range(10):\n",
    "            if seq_start + j < len(train_remaining_tokens):\n",
    "                print(f\"  Step {j}: remaining_tokens = {train_remaining_tokens[seq_start + j]}\")\n",
    "else:\n",
    "    print(\"No obvious sequences found (data is shuffled)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

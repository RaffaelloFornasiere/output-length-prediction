{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 00: Same Prompt Visualization\n",
    "\n",
    "This notebook visualizes the dataset generated using a single prompt template."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T21:39:23.106905Z",
     "start_time": "2025-11-10T21:39:23.104644Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T21:39:23.331912Z",
     "start_time": "2025-11-10T21:39:23.133599Z"
    }
   },
   "source": [
    "# Load data\n",
    "data_dir = Path(\"data\")\n",
    "\n",
    "if not data_dir.exists():\n",
    "    print(\"Data directory not found. Please run generate.py first.\")\n",
    "else:\n",
    "    # Load training data\n",
    "    train_hidden_states = np.load(data_dir / \"train_hidden_states.npy\")\n",
    "    train_remaining_tokens = np.load(data_dir / \"train_remaining_tokens.npy\")\n",
    "    train_token_metadata = np.load(data_dir / \"train_token_metadata.npy\")\n",
    "    \n",
    "    # Load validation data\n",
    "    val_hidden_states = np.load(data_dir / \"val_hidden_states.npy\")\n",
    "    val_remaining_tokens = np.load(data_dir / \"val_remaining_tokens.npy\")\n",
    "    val_token_metadata = np.load(data_dir / \"val_token_metadata.npy\")\n",
    "    \n",
    "    # Load metadata if exists\n",
    "    metadata_path = data_dir / \"metadata.json\"\n",
    "    if metadata_path.exists():\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        print(\"Dataset Metadata:\")\n",
    "        for key, value in metadata.items():\n",
    "            if key != 'prompt_usage_distribution':\n",
    "                print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nData shapes:\")\n",
    "    print(f\"  Train hidden states: {train_hidden_states.shape}\")\n",
    "    print(f\"  Train remaining tokens: {train_remaining_tokens.shape}\")\n",
    "    print(f\"  Val hidden states: {val_hidden_states.shape}\")\n",
    "    print(f\"  Val remaining tokens: {val_remaining_tokens.shape}\")"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/train_hidden_states.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mData directory not found. Please run generate.py first.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Load training data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     train_hidden_states = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain_hidden_states.npy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     train_remaining_tokens = np.load(data_dir / \u001b[33m\"\u001b[39m\u001b[33mtrain_remaining_tokens.npy\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m     train_token_metadata = np.load(data_dir / \u001b[33m\"\u001b[39m\u001b[33mtrain_token_metadata.npy\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/me/output-length-prediction/.venv/lib/python3.13/site-packages/numpy/lib/_npyio_impl.py:454\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[39m\n\u001b[32m    452\u001b[39m     own_fid = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m     fid = stack.enter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    455\u001b[39m     own_fid = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    457\u001b[39m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/train_hidden_states.npy'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data Points"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T21:39:29.644690Z",
     "start_time": "2025-11-10T21:39:29.641836Z"
    }
   },
   "source": [
    "if 'train_hidden_states' in locals():\n",
    "    # Show some random samples\n",
    "    print(\"Random samples from training data:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    sample_indices = np.random.choice(len(train_hidden_states), 10, replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        hidden_state = train_hidden_states[idx]\n",
    "        remaining = train_remaining_tokens[idx]\n",
    "        token_text = train_token_metadata[idx]['token_text']\n",
    "        token_id = train_token_metadata[idx]['token_id']\n",
    "        \n",
    "        print(f\"\\nSample {i+1} (index {idx}):\")\n",
    "        print(f\"  Token: {repr(token_text)} (id={token_id})\")\n",
    "        print(f\"  Remaining tokens: {remaining}\")\n",
    "        print(f\"  Hidden state norm: {np.linalg.norm(hidden_state):.4f}\")\n",
    "        print(f\"  Hidden state mean: {hidden_state.mean():.4f}\")\n",
    "        print(f\"  Hidden state std: {hidden_state.std():.4f}\")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": "## Complete Output Reconstruction\n\nSince the data is stored in generation order (not shuffled), we can reconstruct complete outputs by grouping tokens until we reach remaining_tokens == 0.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "if 'train_hidden_states' in locals():\n    # Reconstruct complete outputs\n    def reconstruct_sequences(remaining_tokens, token_metadata, max_sequences=10):\n        \"\"\"Group tokens into complete generation sequences.\"\"\"\n        sequences = []\n        current_sequence = []\n\n        for i in range(len(remaining_tokens)):\n            token_text = str(token_metadata[i]['token_text'])\n            remaining = remaining_tokens[i]\n\n            current_sequence.append({\n                'token': token_text,\n                'remaining': remaining,\n                'index': i\n            })\n\n            # End of sequence\n            if remaining == 0:\n                sequences.append(current_sequence)\n                current_sequence = []\n\n                if len(sequences) >= max_sequences:\n                    break\n\n        return sequences\n\n    # Reconstruct first 10 complete sequences\n    sequences = reconstruct_sequences(train_remaining_tokens, train_token_metadata, max_sequences=10)\n\n    print(f\"Reconstructed {len(sequences)} complete generation sequences\")\n    print(\"=\"*80)\n\n    for i, seq in enumerate(sequences):\n        # Reconstruct the full output text\n        output_text = ''.join([token['token'] for token in seq])\n        num_tokens = len(seq)\n        start_idx = seq[0]['index']\n        end_idx = seq[-1]['index']\n\n        print(f\"\\nSequence {i+1}:\")\n        print(f\"  Indices: {start_idx} to {end_idx}\")\n        print(f\"  Total tokens: {num_tokens}\")\n        print(f\"  Output: {repr(output_text)}\")\n        print(f\"  Token breakdown:\")\n        for j, token_info in enumerate(seq[:15]):  # Show first 15 tokens\n            print(f\"    {j+1:2d}. {repr(token_info['token']):20s} (remaining: {token_info['remaining']:2d})\")\n        if len(seq) > 15:\n            print(f\"    ... ({len(seq) - 15} more tokens)\")\n        print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
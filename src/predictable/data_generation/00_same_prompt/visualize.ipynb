{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 00: Same Prompt Visualization\n",
    "\n",
    "This notebook visualizes the dataset generated using a single prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_dir = Path(\"data\")\n",
    "\n",
    "if not data_dir.exists():\n",
    "    print(\"Data directory not found. Please run generate.py first.\")\n",
    "else:\n",
    "    # Load training data\n",
    "    train_hidden_states = np.load(data_dir / \"train_hidden_states.npy\")\n",
    "    train_remaining_tokens = np.load(data_dir / \"train_remaining_tokens.npy\")\n",
    "    train_token_metadata = np.load(data_dir / \"train_token_metadata.npy\")\n",
    "    \n",
    "    # Load validation data\n",
    "    val_hidden_states = np.load(data_dir / \"val_hidden_states.npy\")\n",
    "    val_remaining_tokens = np.load(data_dir / \"val_remaining_tokens.npy\")\n",
    "    val_token_metadata = np.load(data_dir / \"val_token_metadata.npy\")\n",
    "    \n",
    "    # Load metadata if exists\n",
    "    metadata_path = data_dir / \"metadata.json\"\n",
    "    if metadata_path.exists():\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        print(\"Dataset Metadata:\")\n",
    "        for key, value in metadata.items():\n",
    "            if key != 'prompt_usage_distribution':\n",
    "                print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nData shapes:\")\n",
    "    print(f\"  Train hidden states: {train_hidden_states.shape}\")\n",
    "    print(f\"  Train remaining tokens: {train_remaining_tokens.shape}\")\n",
    "    print(f\"  Val hidden states: {val_hidden_states.shape}\")\n",
    "    print(f\"  Val remaining tokens: {val_remaining_tokens.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Remaining Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'train_remaining_tokens' in locals():\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Training set distribution\n",
    "    axes[0].hist(train_remaining_tokens, bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[0].set_xlabel('Remaining Tokens')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title(f'Training Set Distribution (n={len(train_remaining_tokens):,})')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Validation set distribution\n",
    "    axes[1].hist(val_remaining_tokens, bins=50, alpha=0.7, edgecolor='black', color='orange')\n",
    "    axes[1].set_xlabel('Remaining Tokens')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title(f'Validation Set Distribution (n={len(val_remaining_tokens):,})')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistics\n",
    "    print(\"\\nStatistics:\")\n",
    "    print(f\"Training set:\")\n",
    "    print(f\"  Min: {train_remaining_tokens.min()}, Max: {train_remaining_tokens.max()}\")\n",
    "    print(f\"  Mean: {train_remaining_tokens.mean():.2f}, Std: {train_remaining_tokens.std():.2f}\")\n",
    "    print(f\"\\nValidation set:\")\n",
    "    print(f\"  Min: {val_remaining_tokens.min()}, Max: {val_remaining_tokens.max()}\")\n",
    "    print(f\"  Mean: {val_remaining_tokens.mean():.2f}, Std: {val_remaining_tokens.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden State Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'train_hidden_states' in locals():\n",
    "    # Compute statistics\n",
    "    train_norms = np.linalg.norm(train_hidden_states, axis=1)\n",
    "    val_norms = np.linalg.norm(val_hidden_states, axis=1)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Norm distributions\n",
    "    axes[0].hist(train_norms, bins=50, alpha=0.5, label='Train', edgecolor='black')\n",
    "    axes[0].hist(val_norms, bins=50, alpha=0.5, label='Val', edgecolor='black')\n",
    "    axes[0].set_xlabel('L2 Norm')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Hidden State Norm Distribution')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Norm vs remaining tokens scatter\n",
    "    scatter_sample = np.random.choice(len(train_remaining_tokens), min(5000, len(train_remaining_tokens)), replace=False)\n",
    "    axes[1].scatter(train_remaining_tokens[scatter_sample], train_norms[scatter_sample], alpha=0.3, s=1)\n",
    "    axes[1].set_xlabel('Remaining Tokens')\n",
    "    axes[1].set_ylabel('Hidden State Norm')\n",
    "    axes[1].set_title('Hidden State Norm vs Remaining Tokens')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'train_token_metadata' in locals():\n",
    "    # Analyze most common tokens\n",
    "    from collections import Counter\n",
    "    \n",
    "    train_tokens = [item['token_text'] for item in train_token_metadata]\n",
    "    token_counts = Counter(train_tokens)\n",
    "    \n",
    "    # Show top 20 most common tokens\n",
    "    print(\"Top 20 most common tokens in training set:\")\n",
    "    for token, count in token_counts.most_common(20):\n",
    "        percentage = (count / len(train_tokens)) * 100\n",
    "        # Handle special characters for display\n",
    "        display_token = repr(token) if token.strip() != token or not token.strip() else token\n",
    "        print(f\"  {display_token:20s}: {count:6d} ({percentage:5.2f}%)\")\n",
    "    \n",
    "    # Visualize token distribution\n",
    "    top_tokens = [t for t, _ in token_counts.most_common(15)]\n",
    "    top_counts = [token_counts[t] for t in top_tokens]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(top_tokens)), top_counts)\n",
    "    plt.xticks(range(len(top_tokens)), [repr(t) if t.strip() != t else t for t in top_tokens], rotation=45, ha='right')\n",
    "    plt.xlabel('Token')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Top 15 Most Common Tokens')\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'train_hidden_states' in locals():\n",
    "    # Show some random samples\n",
    "    print(\"Random samples from training data:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    sample_indices = np.random.choice(len(train_hidden_states), 10, replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        hidden_state = train_hidden_states[idx]\n",
    "        remaining = train_remaining_tokens[idx]\n",
    "        token_text = train_token_metadata[idx]['token_text']\n",
    "        token_id = train_token_metadata[idx]['token_id']\n",
    "        \n",
    "        print(f\"\\nSample {i+1} (index {idx}):\")\n",
    "        print(f\"  Token: {repr(token_text)} (id={token_id})\")\n",
    "        print(f\"  Remaining tokens: {remaining}\")\n",
    "        print(f\"  Hidden state norm: {np.linalg.norm(hidden_state):.4f}\")\n",
    "        print(f\"  Hidden state mean: {hidden_state.mean():.4f}\")\n",
    "        print(f\"  Hidden state std: {hidden_state.std():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 00: Same Prompt Visualization\n",
    "\n",
    "This notebook visualizes the dataset generated using a single prompt template."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T22:19:17.082018Z",
     "start_time": "2025-11-10T22:19:17.069756Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T22:19:17.157288Z",
     "start_time": "2025-11-10T22:19:17.090718Z"
    }
   },
   "source": [
    "# Load data\n",
    "data_dir = Path(\"data\")\n",
    "\n",
    "if not data_dir.exists():\n",
    "    print(\"Data directory not found. Please run generate.py first.\")\n",
    "else:\n",
    "    # Load training data\n",
    "    train_hidden_states = np.load(data_dir / \"train_hidden_states.npy\")\n",
    "    train_remaining_tokens = np.load(data_dir / \"train_remaining_tokens.npy\")\n",
    "    train_token_metadata = np.load(data_dir / \"train_token_metadata.npy\")\n",
    "    \n",
    "    # Load validation data\n",
    "    val_hidden_states = np.load(data_dir / \"val_hidden_states.npy\")\n",
    "    val_remaining_tokens = np.load(data_dir / \"val_remaining_tokens.npy\")\n",
    "    val_token_metadata = np.load(data_dir / \"val_token_metadata.npy\")\n",
    "    \n",
    "    # Load metadata if exists\n",
    "    metadata_path = data_dir / \"metadata.json\"\n",
    "    if metadata_path.exists():\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        print(\"Dataset Metadata:\")\n",
    "        for key, value in metadata.items():\n",
    "            if key != 'prompt_usage_distribution':\n",
    "                print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nData shapes:\")\n",
    "    print(f\"  Train hidden states: {train_hidden_states.shape}\")\n",
    "    print(f\"  Train remaining tokens: {train_remaining_tokens.shape}\")\n",
    "    print(f\"  Val hidden states: {val_hidden_states.shape}\")\n",
    "    print(f\"  Val remaining tokens: {val_remaining_tokens.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Metadata:\n",
      "  dataset_id: 00_same_prompt\n",
      "  description: Dataset generated using a single prompt template\n",
      "  model_name: meta-llama/Llama-3.2-3B-Instruct\n",
      "  prompt_template: Print exactly {count} repetitions of the token \"{word}\". Do not include anything else.\n",
      "  counts_range: [5, 49]\n",
      "  words: ['hello', 'world', 'cat', 'dog', 'python', 'test', 'apple', 'blue', 'sun', 'code']\n",
      "  total_samples: 450\n",
      "  total_tokens: 40522\n",
      "  layers_extracted: last_layer_only\n",
      "\n",
      "Data shapes:\n",
      "  Train hidden states: (36469, 3072)\n",
      "  Train remaining tokens: (36469,)\n",
      "  Val hidden states: (4053, 3072)\n",
      "  Val remaining tokens: (4053,)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data Points"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T22:19:17.178586Z",
     "start_time": "2025-11-10T22:19:17.162219Z"
    }
   },
   "source": [
    "if 'train_hidden_states' in locals():\n",
    "    # Show some random samples\n",
    "    print(\"Random samples from training data:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    sample_indices = np.random.choice(len(train_hidden_states), 10, replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        hidden_state = train_hidden_states[idx]\n",
    "        remaining = train_remaining_tokens[idx]\n",
    "        token_text = train_token_metadata[idx]['token_text']\n",
    "        token_id = train_token_metadata[idx]['token_id']\n",
    "        \n",
    "        print(f\"\\nSample {i+1} (index {idx}):\")\n",
    "        print(f\"  Token: {repr(token_text)} (id={token_id})\")\n",
    "        print(f\"  Remaining tokens: {remaining}\")\n",
    "        print(f\"  Hidden state norm: {np.linalg.norm(hidden_state):.4f}\")\n",
    "        print(f\"  Hidden state mean: {hidden_state.mean():.4f}\")\n",
    "        print(f\"  Hidden state std: {hidden_state.std():.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random samples from training data:\n",
      "============================================================\n",
      "\n",
      "Sample 1 (index 9064):\n",
      "  Token: np.str_('blue') (id=12481)\n",
      "  Remaining tokens: 31\n",
      "  Hidden state norm: 89.3750\n",
      "  Hidden state mean: 0.0536\n",
      "  Hidden state std: 1.6113\n",
      "\n",
      "Sample 2 (index 2296):\n",
      "  Token: np.str_('sun') (id=40619)\n",
      "  Remaining tokens: 25\n",
      "  Hidden state norm: 89.3125\n",
      "  Hidden state mean: -0.0174\n",
      "  Hidden state std: 1.6113\n",
      "\n",
      "Sample 3 (index 36406):\n",
      "  Token: np.str_('sun') (id=40619)\n",
      "  Remaining tokens: 145\n",
      "  Hidden state norm: 89.3750\n",
      "  Hidden state mean: -0.0196\n",
      "  Hidden state std: 1.6123\n",
      "\n",
      "Sample 4 (index 35294):\n",
      "  Token: np.str_('cat') (id=4719)\n",
      "  Remaining tokens: 57\n",
      "  Hidden state norm: 88.8750\n",
      "  Hidden state mean: 0.0319\n",
      "  Hidden state std: 1.6035\n",
      "\n",
      "Sample 5 (index 4469):\n",
      "  Token: np.str_('\\n') (id=198)\n",
      "  Remaining tokens: 30\n",
      "  Hidden state norm: 86.6875\n",
      "  Hidden state mean: -0.0058\n",
      "  Hidden state std: 1.5645\n",
      "\n",
      "Sample 6 (index 6267):\n",
      "  Token: np.str_('\\n') (id=198)\n",
      "  Remaining tokens: 54\n",
      "  Hidden state norm: 84.8750\n",
      "  Hidden state mean: -0.0048\n",
      "  Hidden state std: 1.5312\n",
      "\n",
      "Sample 7 (index 14581):\n",
      "  Token: np.str_('\\n') (id=198)\n",
      "  Remaining tokens: 20\n",
      "  Hidden state norm: 84.3125\n",
      "  Hidden state mean: -0.0074\n",
      "  Hidden state std: 1.5215\n",
      "\n",
      "Sample 8 (index 3942):\n",
      "  Token: np.str_('dog') (id=18964)\n",
      "  Remaining tokens: 39\n",
      "  Hidden state norm: 88.1875\n",
      "  Hidden state mean: 0.0231\n",
      "  Hidden state std: 1.5908\n",
      "\n",
      "Sample 9 (index 6535):\n",
      "  Token: np.str_('\\n') (id=198)\n",
      "  Remaining tokens: 16\n",
      "  Hidden state norm: 83.6250\n",
      "  Hidden state mean: -0.0033\n",
      "  Hidden state std: 1.5088\n",
      "\n",
      "Sample 10 (index 22004):\n",
      "  Token: np.str_('python') (id=12958)\n",
      "  Remaining tokens: 135\n",
      "  Hidden state norm: 90.3750\n",
      "  Hidden state mean: 0.0350\n",
      "  Hidden state std: 1.6299\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": "## Complete Output Reconstruction\n\nSince the data is stored in generation order (not shuffled), we can reconstruct complete outputs by grouping tokens until we reach remaining_tokens == 0.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "if 'train_hidden_states' in locals():\n    # Reconstruct complete outputs\n    def reconstruct_sequences(remaining_tokens, token_metadata, max_sequences=10):\n        \"\"\"Group tokens into complete generation sequences.\"\"\"\n        sequences = []\n        current_sequence = []\n\n        for i in range(len(remaining_tokens)):\n            token_text = str(token_metadata[i]['token_text'])\n            remaining = remaining_tokens[i]\n\n            current_sequence.append({\n                'token': token_text,\n                'remaining': remaining,\n                'index': i\n            })\n\n            # End of sequence\n            if remaining == 0:\n                sequences.append(current_sequence)\n                current_sequence = []\n\n                if len(sequences) >= max_sequences:\n                    break\n\n        return sequences\n\n    # Reconstruct first 10 complete sequences\n    sequences = reconstruct_sequences(train_remaining_tokens, train_token_metadata, max_sequences=10)\n\n    print(f\"Reconstructed {len(sequences)} complete generation sequences\")\n    print(\"=\"*80)\n\n    for i, seq in enumerate(sequences):\n        # Reconstruct the full output text\n        output_text = ''.join([token['token'] for token in seq])\n        num_tokens = len(seq)\n        start_idx = seq[0]['index']\n        end_idx = seq[-1]['index']\n\n        print(f\"\\nSequence {i+1}:\")\n        print(f\"  Indices: {start_idx} to {end_idx}\")\n        print(f\"  Total tokens: {num_tokens}\")\n        print(f\"  Output: {repr(output_text)}\")\n        print(f\"  Token breakdown:\")\n        for j, token_info in enumerate(seq[:15]):  # Show first 15 tokens\n            print(f\"    {j+1:2d}. {repr(token_info['token']):20s} (remaining: {token_info['remaining']:2d})\")\n        if len(seq) > 15:\n            print(f\"    ... ({len(seq) - 15} more tokens)\")\n        print()",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T22:19:17.494355Z",
     "start_time": "2025-11-10T22:19:17.183394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed 10 complete generation sequences\n",
      "================================================================================\n",
      "\n",
      "Sequence 1:\n",
      "  Indices: 0 to 9\n",
      "  Total tokens: 10\n",
      "  Output: 'hello\\nhello\\nhello\\nhello\\nhello<|eot_id|>'\n",
      "  Token breakdown:\n",
      "     1. 'hello'              (remaining:  9)\n",
      "     2. '\\n'                 (remaining:  8)\n",
      "     3. 'hello'              (remaining:  7)\n",
      "     4. '\\n'                 (remaining:  6)\n",
      "     5. 'hello'              (remaining:  5)\n",
      "     6. '\\n'                 (remaining:  4)\n",
      "     7. 'hello'              (remaining:  3)\n",
      "     8. '\\n'                 (remaining:  2)\n",
      "     9. 'hello'              (remaining:  1)\n",
      "    10. '<|eot_id|>'         (remaining:  0)\n",
      "\n",
      "\n",
      "Sequence 2:\n",
      "  Indices: 10 to 19\n",
      "  Total tokens: 10\n",
      "  Output: 'world\\nworld\\nworld\\nworld\\nworld<|eot_id|>'\n",
      "  Token breakdown:\n",
      "     1. 'world'              (remaining:  9)\n",
      "     2. '\\n'                 (remaining:  8)\n",
      "     3. 'world'              (remaining:  7)\n",
      "     4. '\\n'                 (remaining:  6)\n",
      "     5. 'world'              (remaining:  5)\n",
      "     6. '\\n'                 (remaining:  4)\n",
      "     7. 'world'              (remaining:  3)\n",
      "     8. '\\n'                 (remaining:  2)\n",
      "     9. 'world'              (remaining:  1)\n",
      "    10. '<|eot_id|>'         (remaining:  0)\n",
      "\n",
      "\n",
      "Sequence 3:\n",
      "  Indices: 20 to 29\n",
      "  Total tokens: 10\n",
      "  Output: 'cat\\ncat\\ncat\\ncat\\ncat<|eot_id|>'\n",
      "  Token breakdown:\n",
      "     1. 'cat'                (remaining:  9)\n",
      "     2. '\\n'                 (remaining:  8)\n",
      "     3. 'cat'                (remaining:  7)\n",
      "     4. '\\n'                 (remaining:  6)\n",
      "     5. 'cat'                (remaining:  5)\n",
      "     6. '\\n'                 (remaining:  4)\n",
      "     7. 'cat'                (remaining:  3)\n",
      "     8. '\\n'                 (remaining:  2)\n",
      "     9. 'cat'                (remaining:  1)\n",
      "    10. '<|eot_id|>'         (remaining:  0)\n",
      "\n",
      "\n",
      "Sequence 4:\n",
      "  Indices: 30 to 39\n",
      "  Total tokens: 10\n",
      "  Output: 'dog\\ndog\\ndog\\ndog\\ndog<|eot_id|>'\n",
      "  Token breakdown:\n",
      "     1. 'dog'                (remaining:  9)\n",
      "     2. '\\n'                 (remaining:  8)\n",
      "     3. 'dog'                (remaining:  7)\n",
      "     4. '\\n'                 (remaining:  6)\n",
      "     5. 'dog'                (remaining:  5)\n",
      "     6. '\\n'                 (remaining:  4)\n",
      "     7. 'dog'                (remaining:  3)\n",
      "     8. '\\n'                 (remaining:  2)\n",
      "     9. 'dog'                (remaining:  1)\n",
      "    10. '<|eot_id|>'         (remaining:  0)\n",
      "\n",
      "\n",
      "Sequence 5:\n",
      "  Indices: 40 to 49\n",
      "  Total tokens: 10\n",
      "  Output: 'python\\npython\\npython\\npython\\npython<|eot_id|>'\n",
      "  Token breakdown:\n",
      "     1. 'python'             (remaining:  9)\n",
      "     2. '\\n'                 (remaining:  8)\n",
      "     3. 'python'             (remaining:  7)\n",
      "     4. '\\n'                 (remaining:  6)\n",
      "     5. 'python'             (remaining:  5)\n",
      "     6. '\\n'                 (remaining:  4)\n",
      "     7. 'python'             (remaining:  3)\n",
      "     8. '\\n'                 (remaining:  2)\n",
      "     9. 'python'             (remaining:  1)\n",
      "    10. '<|eot_id|>'         (remaining:  0)\n",
      "\n",
      "\n",
      "Sequence 6:\n",
      "  Indices: 50 to 59\n",
      "  Total tokens: 10\n",
      "  Output: 'test\\ntest\\ntest\\ntest\\ntest<|eot_id|>'\n",
      "  Token breakdown:\n",
      "     1. 'test'               (remaining:  9)\n",
      "     2. '\\n'                 (remaining:  8)\n",
      "     3. 'test'               (remaining:  7)\n",
      "     4. '\\n'                 (remaining:  6)\n",
      "     5. 'test'               (remaining:  5)\n",
      "     6. '\\n'                 (remaining:  4)\n",
      "     7. 'test'               (remaining:  3)\n",
      "     8. '\\n'                 (remaining:  2)\n",
      "     9. 'test'               (remaining:  1)\n",
      "    10. '<|eot_id|>'         (remaining:  0)\n",
      "\n",
      "\n",
      "Sequence 7:\n",
      "  Indices: 60 to 69\n",
      "  Total tokens: 10\n",
      "  Output: 'apple\\napple\\napple\\napple\\napple<|eot_id|>'\n",
      "  Token breakdown:\n",
      "     1. 'apple'              (remaining:  9)\n",
      "     2. '\\n'                 (remaining:  8)\n",
      "     3. 'apple'              (remaining:  7)\n",
      "     4. '\\n'                 (remaining:  6)\n",
      "     5. 'apple'              (remaining:  5)\n",
      "     6. '\\n'                 (remaining:  4)\n",
      "     7. 'apple'              (remaining:  3)\n",
      "     8. '\\n'                 (remaining:  2)\n",
      "     9. 'apple'              (remaining:  1)\n",
      "    10. '<|eot_id|>'         (remaining:  0)\n",
      "\n",
      "\n",
      "Sequence 8:\n",
      "  Indices: 70 to 79\n",
      "  Total tokens: 10\n",
      "  Output: 'blue\\nblue\\nblue\\nblue\\nblue<|eot_id|>'\n",
      "  Token breakdown:\n",
      "     1. 'blue'               (remaining:  9)\n",
      "     2. '\\n'                 (remaining:  8)\n",
      "     3. 'blue'               (remaining:  7)\n",
      "     4. '\\n'                 (remaining:  6)\n",
      "     5. 'blue'               (remaining:  5)\n",
      "     6. '\\n'                 (remaining:  4)\n",
      "     7. 'blue'               (remaining:  3)\n",
      "     8. '\\n'                 (remaining:  2)\n",
      "     9. 'blue'               (remaining:  1)\n",
      "    10. '<|eot_id|>'         (remaining:  0)\n",
      "\n",
      "\n",
      "Sequence 9:\n",
      "  Indices: 80 to 89\n",
      "  Total tokens: 10\n",
      "  Output: 'sun\\nsun\\nsun\\nsun\\nsun<|eot_id|>'\n",
      "  Token breakdown:\n",
      "     1. 'sun'                (remaining:  9)\n",
      "     2. '\\n'                 (remaining:  8)\n",
      "     3. 'sun'                (remaining:  7)\n",
      "     4. '\\n'                 (remaining:  6)\n",
      "     5. 'sun'                (remaining:  5)\n",
      "     6. '\\n'                 (remaining:  4)\n",
      "     7. 'sun'                (remaining:  3)\n",
      "     8. '\\n'                 (remaining:  2)\n",
      "     9. 'sun'                (remaining:  1)\n",
      "    10. '<|eot_id|>'         (remaining:  0)\n",
      "\n",
      "\n",
      "Sequence 10:\n",
      "  Indices: 90 to 99\n",
      "  Total tokens: 10\n",
      "  Output: 'code\\ncode\\ncode\\ncode\\ncode<|eot_id|>'\n",
      "  Token breakdown:\n",
      "     1. 'code'               (remaining:  9)\n",
      "     2. '\\n'                 (remaining:  8)\n",
      "     3. 'code'               (remaining:  7)\n",
      "     4. '\\n'                 (remaining:  6)\n",
      "     5. 'code'               (remaining:  5)\n",
      "     6. '\\n'                 (remaining:  4)\n",
      "     7. 'code'               (remaining:  3)\n",
      "     8. '\\n'                 (remaining:  2)\n",
      "     9. 'code'               (remaining:  1)\n",
      "    10. '<|eot_id|>'         (remaining:  0)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
